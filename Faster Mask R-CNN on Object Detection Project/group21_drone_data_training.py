# -*- coding: utf-8 -*-
"""Group21_drone_data_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GZqFExNxMvF7-BpgSyM4Qg04dRK6J0uK

Object detection on the drone data set using Mask R-CNN

Code adapted from MaskRCNN/balloon.py and run_prediction_fromChopped.ipynb

Group21
Mingyong Liu, Emmanuel Obi, Pat Ganesan and Dipak Dulal
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Install packages and set up**"""

# Commented out IPython magic to ensure Python compatibility.
#import packages
import os
import sys
import json
import numpy as np
import skimage.draw
import matplotlib.pyplot as plt
import pickle


# %cd "/content/drive/My Drive/machine_learning_project"
ROOT_DIR = os.getcwd()
print(ROOT_DIR)

#install Mask_RCNN (needed for the first time)
#!git clone https://github.com/akTwelve/Mask_RCNN.git aktwelve_mask_rcnn

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/machine_learning_project/aktwelve_mask_rcnn"
#!pip install -r requirements.txt
#!python setup.py clean --all install

# Import Mask RCNN
from mrcnn import utils
import mrcnn.model as modellib
from mrcnn import visualize
from mrcnn.config import Config

# Check if GPU is used. If not, go to the 'Edit' -> Notebook setting
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""## To perform transfer training, we utilized pre-trained weights from COCO first. The code below import COCO pre-trained weights"""

# Path to trained weights file
COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")
# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_WEIGHTS_PATH):
    utils.download_trained_weights(COCO_WEIGHTS_PATH)


# Directory to save logs and model checkpoints, if not provided
# through the command line argument --logs
DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, "logs")

"""Code below sets up the configurations for training and testing. The number of classes was set to 6 because we have 5 classes plus 1 background. The STEPS_PER_EPOCH was set to 150, and ResNet50 backbone was used. The other parameters were kept the same as the solution provided by Dr. Yan."""

############################################################
#  Configurations
############################################################
class DroneConfig(Config):
    """Configuration for training on the Drone dataset.
    Derives from the base Config class and overrides some values.
    """
    # Give the configuration a recognizable name
    NAME = "drone_cfg"

   # number of classes (background + others)
    NUM_CLASSES = 1 + 5
    # number of training steps per epoch
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    STEPS_PER_EPOCH=150
    ###################################################################################
    BACKBONE = "resnet50"
    #BACKBONE = "resnet101"

    #Turn off the USE_MINI_MASK (Important!!)
    USE_MINI_MASK = False

    
    # Length of square anchor side in pixels
    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)
    
    # Non-max suppression threshold to filter RPN proposals.
    # You can increase this during training to generate more propsals.
    RPN_NMS_THRESHOLD = 0.6
    
    # How many anchors per image to use for RPN training
    RPN_TRAIN_ANCHORS_PER_IMAGE = 512
    
    # Percent of positive ROIs used to train classifier/mask heads
    ROI_POSITIVE_RATIO = 0.6
    
#     IMAGE_MIN_DIM = 512
#     IMAGE_MAX_DIM = 1024
    
    # Number of ROIs per image to feed to classifier/mask heads
    # The Mask RCNN paper uses 512 but often the RPN doesn't generate
    # enough positive proposals to fill this and keep a positive:negative
    # ratio of 1:3. You can increase the number of proposals by adjusting
    # the RPN NMS threshold.
    TRAIN_ROIS_PER_IMAGE = 512
    
    # Max number of final detections
    DETECTION_MAX_INSTANCES = 100
    
    # Minimum probability value to accept a detected instance
    # ROIs below this threshold are skipped
    DETECTION_MIN_CONFIDENCE = 0.7

    # Non-maximum suppression threshold for detection
    DETECTION_NMS_THRESHOLD = 0.4
    pass


config = DroneConfig()
config.display()

"""Code below creates a DroneDataset class by inheriting the utils.Dataset class. The first function load_drone_data takes in pictures and their associated VIA annotations. The second and third functions draw a box and polygon mask according to annotations, respectively."""

############################################################
#  Dataset
############################################################

class DroneDataset(utils.Dataset):

    def load_drone_data(self, dataset_dir, subset):
        """Load the Drone dataset.
        dataset_dir: Root directory of the dataset.
        subset: Subset to load: train or val
        """
        # define one class
        self.add_class("classes", 1, "Head")
        self.add_class("classes", 2, "Thread")
        self.add_class("classes", 3, "Nut")
        self.add_class("classes", 4, "Washer")
        self.add_class("classes", 5, "Pin")

        
        # define data locations
        images_dir = dataset_dir
        image_files = [file for file in os.listdir(images_dir) if file.endswith('.jpg')]
        image_num = len(image_files)

        # Split data into training, testing and validation data subsets at a 0.7:0.15:0.15 ratio
        assert subset in ['Train','Test','Val']
        if subset == 'Train':
            image_ids = image_files[:int(0.7 * image_num)]
        elif subset == 'Val':
            image_ids = image_files[int(0.7 * image_num):int(0.85 * image_num)]
        elif subset == 'Test':
            image_ids = image_files[int(0.95 * image_num):]

        # Load annotations
        # VGG Image Annotator saves each image in the form:
        # { 'filename': '28503151_5b5b7ec140_b.jpg',
        #   'regions': {
        #       '0': {
        #           'region_attributes': {},
        #           'shape_attributes': {
        #               'all_points_x': [...],
        #               'all_points_y': [...],
        #               'name': 'polygon'}},
        #       ... more regions ...
        #   },
        #   'size': 100202
        # }

        # Extract the x and y coordinates of each region
        annotations = json.load(open(os.path.join(dataset_dir, "new_HQ_annotations.json")))
        annotations = list(annotations.values())  # don't need the dict keys

        # Extract the information from JSON file according to the data subset
        annotations = [a for a in annotations if (a['filename'] in image_ids)]
   

        # Add images
        for a in annotations:
            # Get the x, y coordinaets of points of the polygons that make up
            # the outline of each object instance.

            polygons = [r['shape_attributes'] for r in a['regions']]

            #class is saved in region_attributes
            classes = [r['region_attributes']['class'] for r in a['regions']]

            # load_mask() needs the image size to convert polygons to masks
            image_path = os.path.join(dataset_dir, a['filename'])
            image = skimage.io.imread(image_path)
            height, width = image.shape[:2]

            self.add_image(
                "classes",
                image_id=a['filename'],  # use file name as a unique image id
                classes = classes,
                path=image_path,
                width=width, height=height,
                polygons=polygons)
            
    def extract_boxes(self, image_id):
        info = self.image_info[image_id]
        return info['polygons'], info['width'], info['height']

    # load the masks for an image
    def load_mask(self, image_id):
        boxes, w, h = self.extract_boxes(image_id)
        #print(boxes)
        
        # create one array for all masks, each on a different channel
        image_info = self.image_info[image_id]
        masks = np.zeros([h, w, len(boxes)], dtype='uint8')
        # create masks
        class_ids = list()
        regions = image_info['polygons']
        for i,r in enumerate(regions):
            #class_label = r['region_attributes']['class']
            class_label = image_info['classes'][i]
            shape = r
            assert (shape['name'] == 'polygon'), 'Shape is not polygon but ' + shape['name']
            all_points_x = shape['all_points_x']
            all_points_y = shape['all_points_y']
            assert (len(all_points_x) == len(all_points_y)), 'all_points_x != all_points_y'
            rr, cc = skimage.draw.polygon(all_points_y, all_points_x)
            masks[rr, cc, i] = 1
            class_ids.append(self.class_names.index(class_label))
        return masks, np.asarray(class_ids, dtype='int32')

"""## **Training**"""

# Training dataset
train_set = DroneDataset()
train_folder = "/content/drive/My Drive/machine_learning_project/split_pictures_112620/"
train_set.load_drone_data(train_folder, subset='Train')
train_set.prepare()
print('train: %d' % len(train_set.image_ids))

# Validation dataset
val_set = DroneDataset()
val_folder = "/content/drive/My Drive/machine_learning_project/split_pictures_112620/"
val_set.load_drone_data(val_folder, subset = "Val")
val_set.prepare()
print('val: %d' % len(val_set.image_ids))

#Create a model for training
model = modellib.MaskRCNN(mode="training", config=config, model_dir = DEFAULT_LOGS_DIR)

#initiate the model with COCO pre-trained weights
model.load_weights(COCO_WEIGHTS_PATH, by_name=True,
                   exclude=["mrcnn_class_logits", "mrcnn_bbox_fc", "mrcnn_bbox", "mrcnn_mask"])

"""After weights from the pre-trained model (COCO) were loaded, we performed transfer training. This is needed because the original COCO data set does not have items that we are going to detect (such as Header, Washer etc). Our training strategy was to first train the classifier (layers='heads') of a model while freezing all other layers, then train the other layers (layers='all'). The readouts that we used to determine the performance of a model were 'loss' and 'val_loss'. After trying numerous settings for epochs and learning_rate, we constantly found that the loss was lower when we only trained the heads.

Code below is an example setting that we tried, wherein we tried the heads for 60 epoches using the default learning rate (0.001)
"""

# Train the head branches
# Passing layers="heads" freezes all layers except the head layers.
model.train(train_set, val_set,
            learning_rate=config.LEARNING_RATE, 
            epochs= 60,
            layers='heads')

# Train the 4+ layer
#model.train(train_set, val_set,
#            learning_rate=config.LEARNING_RATE, 
#            epochs= 60,
#            layers='4+')

#Train the body
#model.train(train_set, val_set,
#            learning_rate=config.LEARNING_RATE / 10, 
#           epochs= 80,
#            layers='all')

"""After training, loss functions for each epoch were plotted. The weight file of the epoch with the lowest loss was chosen to detect items from testing pictures."""

# Commented out IPython magic to ensure Python compatibility.
#visualize the epoch loss
# %load_ext tensorboard
#point tensorboard to where all the weight (h5) files are 
# %tensorboard --logdir="/content/drive/MyDrive/machine_learning_project/logs/drone_20201202_headonly"

"""## **Detection from test data set**"""

#inference mode
model = modellib.MaskRCNN(mode="inference", config=config, model_dir=DEFAULT_LOGS_DIR)


# load the weight file of a model with the lowest loss
model_path = os.path.join(ROOT_DIR, "drone_weights_120120.h5")
print("Loading weights from ", model_path)
model.load_weights(model_path, by_name=True)

"""After loading the weights, we used the new model to detect items from test dataset"""

# Create Test dataset
test_set = DroneDataset()
test_folder = "/content/drive/My Drive/machine_learning_project/split_pictures_112620/"
test_set.load_drone_data(test_folder, subset = "Test")
test_set.prepare()

#Run detection on test data

results = []
APs_50 = {}
APs_75 = {}
APs_90 = {}
ARs_50 = {}
ARs_75 = {}
ARs_90 = {}

for image_id in test_set.image_ids:
    print('image_id =', image_id)

    #Load the ground truth pictures
    image, image_meta, gt_class_id, gt_bbox, gt_mask =\
    modellib.load_image_gt(test_set, config, image_id)

    #run detection using our model
    res = model.detect([image], verbose=0)[0]
    results.append(res)
    
    #calculate average precision (AP) at different iou thresholds
    AP_50, precisions, recalls, overlaps =\
    utils.compute_ap(gt_bbox, gt_class_id, gt_mask, res['rois'], res['class_ids'], res['scores'], res['masks'], iou_threshold=0.5)
    APs_50[image_id] = AP_50

    AP_75, precisions, recalls, overlaps =\
    utils.compute_ap(gt_bbox, gt_class_id, gt_mask, res['rois'], res['class_ids'], res['scores'], res['masks'], iou_threshold=0.75)
    APs_75[image_id] = AP_75

    AP_90, precisions, recalls, overlaps =\
    utils.compute_ap(gt_bbox, gt_class_id, gt_mask, res['rois'], res['class_ids'], res['scores'], res['masks'], iou_threshold=0.90)
    APs_90[image_id] = AP_90
    
    #calculate average recall (AR) at different iou thresholds
    AR_50, _ = utils.compute_recall(res['rois'], gt_bbox, iou = 0.5)
    ARs_50[image_id] = AR_50

    AR_75, _ = utils.compute_recall(res['rois'], gt_bbox, iou = 0.75)
    ARs_75[image_id] = AR_75

    AR_90, _ = utils.compute_recall(res['rois'], gt_bbox, iou = 0.90)
    ARs_90[image_id] = AR_90

#Average precisions
print('mAP_iou0.50 =',np.mean(list(APs_50.values())))
print('mAP_iou0.75 =',np.mean(list(APs_75.values())))
print('mAP_iou0.90 =',np.mean(list(APs_90.values())))

#plot the Average precisions
plt.boxplot([list(APs_50.values()),list(APs_75.values()),list(APs_90.values())])
plt.xticks([1, 2, 3], ['iou=0.5', 'iou=0.75', 'iou=0.9'])
plt.show()

#Rank the img id based on average precisions at iou_threshold = 0.5
rank = sorted(APs_50.items(), key = lambda x: x[1], reverse = True)
print(rank[0:15])

#Average recall
print('mAR_iou0.50 =',np.mean(list(ARs_50.values())))
print('mAR_iou0.75 =',np.mean(list(ARs_75.values())))
print('mAR_iou0.90 =',np.mean(list(ARs_90.values())))

#plot the Average recall
plt.boxplot([list(ARs_50.values()),list(ARs_75.values()),list(ARs_90.values())])
plt.xticks([1, 2, 3], ['iou=0.5', 'iou=0.75', 'iou=0.9'])
plt.show()

"""## **Compare human annotation and predicted detection for the same picture**"""

#Choose a representative plot (img_id 126)
#other best img ids are 1,28,41,70,85,131,263,209,283 and 307, which we include in our report
image_id = 126
print(test_set.image_info[image_id]['path'])
image = test_set.load_image(image_id)
mask, class_ids = test_set.load_mask(image_id)
#visualize.display_top_masks(image, mask, class_ids, train_set.class_names)

from skimage import io
from matplotlib import pyplot as plt

io.imshow(image)
plt.show()

#show the human annotation of this picture (ground-truth)
bbox = utils.extract_bboxes(mask)
visualize.display_instances(image, bbox, mask, class_ids, test_set.class_names)

def get_ax(rows=1, cols=1, size=16): # increase size to see components more clearly !!!
  _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))
  return ax

#Plot the predicted detection by our model
r = results[image_id]
image, image_meta, gt_class_id, gt_bbox, gt_mask =\
    modellib.load_image_gt(test_set, config, image_id)

ax = get_ax(1)
visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], 
                            test_set.class_names, r['scores'], ax=ax,
                            title="Predictions")